# Nash Q-Learning Configuration for LineWorld Environment

# Experiment tracking
project: rl-practice
run_name: nash-ql-lineworld
entity: null

# Environment
env_id: line_world
seed: 42
env_kwargs:
  n_agents: 2
  grid_length: 7
  max_steps: 80
  goal_positions: null  # Default: agents start at left, goals near right
  step_penalty: -0.02
  goal_reward: 1.0
  shared_goal_bonus: 0.5
  collision_penalty: -0.1

# Training hyperparameters
total_episodes: 5000
max_steps_per_episode: 120
gamma: 0.95  # Discount factor
alpha: 0.1   # Learning rate
epsilon_start: 1.0
epsilon_end: 0.05
epsilon_decay: 0.0008

# Logging & checkpoints
log_interval: 50
checkpoint_interval: 500
checkpoint_dir: Nash-QL/checkpoints
save_best: true
log_level: INFO
log_to_file: false
log_file: Nash-QL/logs/nash_ql.log
log_to_console: true

# Inference
inference_model_path: Nash-QL/checkpoints/best.pt
episodes: 5

# Device
device: auto  # auto, cpu, or cuda
